# Tool Call Streaming Fix

## Issue
Tool calls were appearing in the UI only after completion, not as they were being generated by the LLM. This differed from the Python backend behavior where tool calls stream in real-time.

## Root Cause
In Vercel AI SDK v4.x, the `toolCallStreaming` parameter is **disabled by default**. This means:
- Tool call arguments are only sent to the client after the entire tool call is complete
- The UI shows tool calls suddenly appearing, not gradually streaming in
- This creates a less responsive user experience compared to the Python backend

## Solution
Enable `toolCallStreaming: true` in the `streamText()` configuration:

```typescript
const result = streamText({
  model,
  messages,
  tools,
  toolCallStreaming: true, // Enable real-time tool call streaming
  // ... other options
});
```

## How It Works

With `toolCallStreaming: true`, the Vercel AI SDK streams tool calls in real-time:

1. **`tool-call-streaming-start`** - Sent when a tool call begins
   - Contains: `toolCallId`, `toolName`
   - UI can show "Calling tool X..."

2. **`tool-call-delta`** - Sent as arguments stream in
   - Contains: `toolCallId`, `toolName`, `argsTextDelta`
   - UI can show arguments being built up in real-time

3. **`tool-call`** - Sent when tool call is complete
   - Contains: `toolCallId`, `toolName`, `args` (complete)
   - UI can show final tool call with all arguments

## Version Differences

- **AI SDK v4.x**: `toolCallStreaming` is **disabled by default** (must opt-in)
- **AI SDK v5.x**: `toolCallStreaming` is **enabled by default** (opt-out)

## Implementation Location

File: `backend-ts/src/lib/agents/routine.ts`

```typescript
const result = streamText({
  // ... other config
  toolCallStreaming: true, // Line ~335
  // ... other config
});
```

## Testing

To verify tool call streaming is working:

1. Start the backend: `npm run dev` in `backend-ts/`
2. Open the frontend and start a chat
3. Ask a question that triggers a tool call (e.g., "What brain regions are in the hippocampus?")
4. Observe that:
   - Tool name appears immediately when LLM starts generating the call
   - Arguments appear gradually as they're generated
   - Tool execution begins as soon as the call is complete

## Related Documentation

- [Vercel AI SDK Tool Call Streaming](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage#tool-call-streaming)
- [streamText API Reference](https://sdk.vercel.ai/docs/reference/ai-sdk-core/stream-text)
- Python backend comparison: `backend/src/neuroagent/agent_routine.py` (lines 280-350)

## Impact

This change brings the TypeScript backend's streaming behavior in line with the Python backend, providing:
- Immediate visual feedback when tools are being called
- Better user experience with progressive disclosure
- Consistency across both backend implementations

"""Literature Search tool."""

import json
import logging
from collections import defaultdict
from typing import Any, ClassVar

from httpx import AsyncClient
from openai import AsyncOpenAI
from pydantic import BaseModel, Field, field_validator

from neuroagent.tools.autogenerated_types.literature.models import ParagraphMetadata
from neuroagent.tools.autogenerated_types.literature.schemas import (
    ArticleTypes,
    Authors,
    DateFrom,
    DateTo,
    Journals,
    RetrievalRetrievalGetParams,
)
from neuroagent.tools.base_tool import BaseMetadata, BaseTool

logger = logging.getLogger(__name__)


class LiteratureSearchInput(BaseModel):
    """Inputs of the literature search API."""

    user_message: str = Field(
        description=("Message of the user that triggered the tool call.")
    )
    max_article_number: int = Field(
        default=5, ge=1, le=10, description="Maximum number of articles to return."
    )
    article_types: list[str] | None = Field(
        default=None,
        description="Filter that restricts the type of retrieved articles.",
    )
    authors: list[str] | None = Field(
        default=None,
        description="Filter that restricts the authors of retrieved articles.",
    )
    journals: list[str] | None = Field(
        default=None,
        description="Filter that restricts the journal of publication of retrieved articles. Should be the ISSN of the journal.",
    )
    date_from: str | None = Field(
        default=None, description="Publication date lowerbound. Format YYYY-MM-DD"
    )
    date_to: str | None = Field(
        default=None, description="Publication date upperbound. Format YYYY-MM-DD"
    )


class LiteratureSearchMetadata(BaseMetadata):
    """Metadata class for LiteratureSearchTool."""

    literature_search_url: str
    token: str
    retriever_k: int
    use_reranker: bool
    openai_client: AsyncOpenAI


class ArticleSelection(BaseModel):
    """Output class for the LLM."""

    sources: list[int] = Field(
        default_factory=list,
        description="List of selected articles, referenced by their field `source`.",
    )


class ParagraphOutput(BaseModel):
    """Paragraph parameters."""

    section: str | None
    paragraph: str
    reranking_score: float | None

    @field_validator("paragraph", mode="before")
    @classmethod
    def truncate_paragraph(cls, text: str) -> str:
        """Truncate long test."""
        max_length = 10000
        if isinstance(text, str) and len(text) > max_length:
            return text[:max_length]
        return text


class ArticleOutput(BaseModel):
    """Results of the Litterature Search API."""

    article_title: str
    article_authors: list[str]
    article_doi: str | None
    pubmed_id: str | None
    date: str | None
    article_type: str | None
    journal_issn: str | None
    journal_name: str | None
    cited_by: int | None
    impact_factor: float | None
    abstract: str | None
    paragraphs: list[ParagraphOutput]
    source: int

    @field_validator("abstract", mode="before")
    @classmethod
    def truncate_text(cls, text: str) -> str:
        """Truncate long test."""
        max_length = 10000
        if isinstance(text, str) and len(text) > max_length:
            return text[:max_length]
        return text


class LiteratureSearchToolOutput(BaseModel):
    """Output schema for the literature search tool."""

    articles: list[ArticleOutput] | None = None
    error: str | None = None


class LiteratureSearchTool(BaseTool):
    """Class defining the Literature Search logic."""

    name: ClassVar[str] = "literature-search-tool"
    name_frontend: ClassVar[str] = "Literature Search"
    description_frontend: ClassVar[
        str
    ] = """Search through scientific papers to find relevant information. This tool is particularly useful for:
    • Finding scientific facts about neuroscience and medicine
    • Getting information from peer-reviewed articles
    • Accessing research findings and academic knowledge

    The search will return relevant paragraphs from scientific papers along with their source information."""
    description: ClassVar[
        str
    ] = """Searches the scientific literature. The tool should be used to gather general scientific knowledge. It is best suited for questions about neuroscience and medicine that are not about morphologies.
        It returns a list of scientific articles that have paragraphs matching the query alongside with the metadata of the articles they were extracted from.
        Always display the articles according to the following schema:
                ```
                Article Title: The title of the article
                    - Authors: The list of authors of the article
                    - DOI: The DOI of the article
                    - Published Date: The publication date of the article
                    - Citations: The citation count, if available.
                    - Journal: The journal of publication, if available
                    - Brain Region Reference: The specific brain region or subregion mentioned in the article.
                    - Topic Relevance Explanation: A brief explanation of how the article's content relates directly to the requested topic.
                ```
                """
    metadata: LiteratureSearchMetadata
    input_schema: LiteratureSearchInput

    async def arun(self) -> LiteratureSearchToolOutput:
        """Async search the scientific literature and returns citations.

        Returns
        -------
            List of paragraphs and their metadata
        """
        logger.info(
            f"Entering literature search tool. Inputs: {self.input_schema.user_message=}"
        )

        params = RetrievalRetrievalGetParams(
            query=self.input_schema.user_message,
            article_types=ArticleTypes(root=self.input_schema.article_types),
            authors=Authors(root=self.input_schema.authors),
            journals=Journals(root=self.input_schema.journals),
            date_from=DateFrom(root=self.input_schema.date_from),
            date_to=DateTo(root=self.input_schema.date_to),
            retriever_k=self.metadata.retriever_k,
            use_reranker=self.metadata.use_reranker,
            reranker_k=100,
        )

        # Send the request
        response = await self.metadata.httpx_client.get(
            self.metadata.literature_search_url + "/retrieval/",
            headers={"Authorization": f"Bearer {self.metadata.token}"},
            params={
                k: v for k, v in params.model_dump().items() if v is not None
            },  # .model_dump(exclude_none=True) doesn't work since root models are not BaseModel.
            timeout=None,
        )
        if response.status_code != 200:
            return LiteratureSearchToolOutput(error=response.text)

        articles = self._aggregate_paragraphs(output=response.json(), max_articles=10)
        messages = [
            {
                "role": "system",
                "content": f"""You are an expert neuroscience literature analyst. You will receive:
            • QUERY: the user’s question.
            • ARTICLES: a JSON array of candidate articles, each with at least “article_id”, “abstract” and some "paragraphs".

            Your job is to pick **only** those articles (up to {self.input_schema.max_article_number}) that meet **both** of these criteria:
            1. **Region match**: The article explicitly mentions the target brain region **or** one of its recognized subregions in relation to the query. Mentions of only a broader parent region does **not** count.
            2. **Topic match**: The article directly addresses the user’s requested topic.

            **Output** an array of the sources from the selected articles (list of integer), sorted by descending relevance.
            Articles mentioning specifically the target brain region or sub-regions are more relevant than the ones mentioning other brain regions as well.
            If none qualify, return an empty array.
            Be very selective, only include articles that fully satisfy both conditions.""",
            },
            {
                "role": "user",
                "content": f"""QUERY: {self.input_schema.user_message}
            ARTICLES: {json.dumps([article.model_dump() for article in articles])}""",
            },
        ]

        llm_outputs = await self.metadata.openai_client.beta.chat.completions.parse(
            messages=messages,  # type: ignore
            model="gpt-4o-mini",
            response_format=ArticleSelection,
        )

        return self._process_output(
            llm_outputs=llm_outputs.choices[0].message.parsed,  # type: ignore
            articles=articles,
            max_article_number=self.input_schema.max_article_number,
        )

    @staticmethod
    def _aggregate_paragraphs(
        output: list[dict[str, Any]], max_articles: int
    ) -> list[ArticleOutput]:
        """Process output."""
        paragraphs_metadata = [ParagraphMetadata(**paragraph) for paragraph in output]

        # Aggregate the paragraphs into articles
        articles: dict[str, list[ParagraphOutput]] = defaultdict(list)
        article_metadata: dict[str, ParagraphMetadata] = {}

        i = 0
        while len(articles) < max_articles and i < len(paragraphs_metadata):
            paragraph = paragraphs_metadata[i]
            articles[paragraph.article_id].append(
                ParagraphOutput(
                    section=paragraph.section,
                    paragraph=paragraph.paragraph,
                    reranking_score=paragraph.reranking_score,
                )
            )
            if paragraph.article_id not in article_metadata:
                article_metadata[paragraph.article_id] = paragraph
            i += 1

        paragraphs_output = [
            ArticleOutput(
                article_title=article_metadata[article_id].article_title,
                article_authors=article_metadata[article_id].article_authors,
                paragraphs=articles[article_id],
                article_doi=article_metadata[article_id].article_doi,
                pubmed_id=article_metadata[article_id].pubmed_id,
                date=article_metadata[article_id].date,
                article_type=article_metadata[article_id].article_type,
                journal_issn=article_metadata[article_id].journal_issn,
                journal_name=article_metadata[article_id].journal_name,
                cited_by=article_metadata[article_id].cited_by,
                impact_factor=article_metadata[article_id].impact_factor,
                abstract=article_metadata[article_id].abstract,
                source=i,
            )
            for i, article_id in enumerate(articles.keys())
        ]
        return paragraphs_output

    @staticmethod
    def _process_output(
        llm_outputs: ArticleSelection,
        articles: list[ArticleOutput],
        max_article_number: int,
    ) -> LiteratureSearchToolOutput:
        """Turn the LLM output into a tool output."""
        selected_articles: list[ArticleOutput] = []
        for output in llm_outputs.sources:
            # if we are potentially missing articles keep appending
            if len(selected_articles) < max_article_number:
                # Get the article that matches the current article_id
                try:
                    selected_articles.append(
                        next(
                            (
                                article
                                for article in articles
                                if article.source == output
                            )
                        )
                    )
                except StopIteration:
                    continue

            # When we have enough break
            else:
                break
        return LiteratureSearchToolOutput(articles=selected_articles)

    @classmethod
    async def is_online(
        cls, *, httpx_client: AsyncClient, literature_search_url: str
    ) -> bool:
        """Check if the tool is online."""
        response = await httpx_client.get(
            literature_search_url,
        )
        return response.status_code == 200
